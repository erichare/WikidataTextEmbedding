{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Wikidata dump ZIP file and saving the IDs of entities and properties to a JSON file (Only the ones connected to the English Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "999 Lines Processed \t Line Process Avg: 333 items/sec \t Memory Usage Avg: 881.89 MB\n",
      "3999 Lines Processed \t Line Process Avg: 666 items/sec \t Memory Usage Avg: 1093.85 MB\n",
      "5999 Lines Processed \t Line Process Avg: 635 items/sec \t Memory Usage Avg: 1218.73 MB\n",
      "8999 Lines Processed \t Line Process Avg: 722 items/sec \t Memory Usage Avg: 1289.64 MB\n",
      "11999 Lines Processed \t Line Process Avg: 769 items/sec \t Memory Usage Avg: 1339.02 MB\n",
      "14999 Lines Processed \t Line Process Avg: 793 items/sec \t Memory Usage Avg: 1277.33 MB\n",
      "15999 Lines Processed \t Line Process Avg: 719 items/sec \t Memory Usage Avg: 1359.25 MB\n",
      "17999 Lines Processed \t Line Process Avg: 713 items/sec \t Memory Usage Avg: 1431.32 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from wikidata_dumpreader import WikidataDumpReader\n",
    "from wikidataDB import WikidataID\n",
    "from multiprocessing import Lock\n",
    "\n",
    "FILEPATH = '../data/Wikidata/latest-all.json.bz2'\n",
    "BATCH_SIZE = 1000\n",
    "NUM_PROCESSES = 4\n",
    "language = 'en'\n",
    "sqlitDBlock = Lock()\n",
    "\n",
    "wikidata = WikidataDumpReader(FILEPATH, num_processes=NUM_PROCESSES, batch_size=BATCH_SIZE, skiplines=0)\n",
    "\n",
    "\n",
    "def in_en_wiki(item):\n",
    "    return ('sitelinks' in item) and (f'{language}wiki' in item['sitelinks']) and ((language in item['labels']) or ('mul' in item['labels'])) and ((language in item['descriptions']) or ('mul' in item['descriptions']))\n",
    "\n",
    "bulk_ids = []\n",
    "def count_types(item):\n",
    "    global bulk_ids\n",
    "\n",
    "    if item is not None:\n",
    "        if in_en_wiki(item):\n",
    "            bulk_ids.append({'id': item['id'], 'in_wikipedia': True, 'is_property': False})\n",
    "\n",
    "            for pid,claim in item.get('claims', {}).items():\n",
    "                bulk_ids.append({'id': pid, 'in_wikipedia': False, 'is_property': True})\n",
    "\n",
    "                for c in claim:\n",
    "                    if ('mainsnak' in c) and ('datavalue' in c['mainsnak']):\n",
    "                        if (c['mainsnak'].get('datatype', '') == 'wikibase-item'):\n",
    "                            id = c['mainsnak']['datavalue']['value']['id']\n",
    "                            bulk_ids.append({'id': id, 'in_wikipedia': False, 'is_property': False})\n",
    "\n",
    "                        elif (c['mainsnak'].get('datatype', '') == 'wikibase-property'):\n",
    "                            id = c['mainsnak']['datavalue']['value']['id']\n",
    "                            bulk_ids.append({'id': id, 'in_wikipedia': False, 'is_property': True})\n",
    "\n",
    "                        elif (c['mainsnak'].get('datatype', '') == 'quantity') and (c['mainsnak']['datavalue']['value'].get('unit', '1') != '1'):\n",
    "                            id = c['mainsnak']['datavalue']['value']['unit'].rsplit('/', 1)[1]\n",
    "                            bulk_ids.append({'id': id, 'in_wikipedia': False, 'is_property': False})\n",
    "\n",
    "                    if 'qualifiers' in c:\n",
    "                        for pid, qualifier in c['qualifiers'].items():\n",
    "                            bulk_ids.append({'id': pid, 'in_wikipedia': False, 'is_property': True})\n",
    "                            for q in qualifier:\n",
    "                                if ('datavalue' in q):\n",
    "                                    if (q['datatype'] == 'wikibase-item'):\n",
    "                                        id = q['datavalue']['value']['id']\n",
    "                                        bulk_ids.append({'id': id, 'in_wikipedia': False, 'is_property': False})\n",
    "\n",
    "                                    elif(q['datatype'] == 'wikibase-property'):\n",
    "                                        id = q['datavalue']['value']['id']\n",
    "                                        bulk_ids.append({'id': id, 'in_wikipedia': False, 'is_property': True})\n",
    "\n",
    "                                    elif (q['datatype'] == 'quantity') and (q['datavalue']['value'].get('unit', '1') != '1'):\n",
    "                                        id = q['datavalue']['value']['unit'].rsplit('/', 1)[1]\n",
    "                                        bulk_ids.append({'id': id, 'in_wikipedia': False, 'is_property': False})\n",
    "\n",
    "            with sqlitDBlock:\n",
    "                if len(bulk_ids) > BATCH_SIZE:\n",
    "                    worked = WikidataID.add_bulk_ids(bulk_ids)\n",
    "                    if worked:\n",
    "                        bulk_ids = []\n",
    "\n",
    "async def run_processor():\n",
    "    await wikidata.run(count_types, max_iterations=None, verbose=True)\n",
    "\n",
    "await run_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding entities (label, description, claims, and aliases) of IDs found in WikidataID to WikidataEntity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from wikidata_dumpreader import WikidataDumpReader\n",
    "from wikidataDB import WikidataID, WikidataEntity\n",
    "from multiprocessing import Lock\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "FILEPATH = '../data/Wikidata/latest-all.json.bz2'\n",
    "BATCH_SIZE = 1000\n",
    "NUM_PROCESSES = 6\n",
    "skiplines = 0\n",
    "wikidata = WikidataDumpReader(FILEPATH, num_processes=NUM_PROCESSES, batch_size=BATCH_SIZE, skiplines=skiplines)\n",
    "\n",
    "def remove_keys(data, keys_to_remove=['hash', 'property', 'numeric-id', 'qualifiers-order']):\n",
    "    if isinstance(data, dict):\n",
    "        return {\n",
    "            key: remove_keys(value, keys_to_remove)\n",
    "            for key, value in data.items() if key not in keys_to_remove\n",
    "        }\n",
    "    elif isinstance(data, list):\n",
    "        return [remove_keys(item, keys_to_remove) for item in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def get_claims(item):\n",
    "    claims = {}\n",
    "    if 'claims' in item:\n",
    "        for pid,x in item['claims'].items():\n",
    "            pid_claims = []\n",
    "            for i in x:\n",
    "                if (i['type'] == 'statement') and (i['rank'] != 'deprecated'):\n",
    "                    pid_claims.append({\n",
    "                        'mainsnak': remove_keys(i['mainsnak']) if 'mainsnak' in i else {},\n",
    "                        'qualifiers': remove_keys(i['qualifiers']) if 'qualifiers' in i else {},\n",
    "                        'rank': i['rank']\n",
    "                    })\n",
    "            if len(pid_claims) > 0:\n",
    "                claims[pid] = pid_claims\n",
    "    return claims\n",
    "\n",
    "def get_aliases(item):\n",
    "    aliases = set()\n",
    "    if language in item['aliases']:\n",
    "        aliases = set([x['value'] for x in item['aliases'][language]])\n",
    "    if 'mul' in item['aliases']:\n",
    "        aliases = aliases | set([x['value'] for x in item['aliases']['mul']])\n",
    "    return list(aliases)\n",
    "\n",
    "data_batch = []\n",
    "progressbar = tqdm(total=12327824, desc=\"Running...\")\n",
    "progressbar.update(skiplines)\n",
    "sqlitDBlock = Lock()\n",
    "language = 'en'\n",
    "def save_entites_to_sqlite(item):\n",
    "    global data_batch\n",
    "\n",
    "    if item is not None:\n",
    "        if WikidataID.get_id(item['id']):\n",
    "            label = item['labels'][language]['value'] if (language in item['labels']) else (item['labels']['mul']['value'] if ('mul' in item['labels']) else '')\n",
    "            description = item['descriptions'][language]['value'] if (language in item['descriptions']) else (item['descriptions']['mul']['value'] if ('mul' in item['descriptions']) else '')\n",
    "            aliases = get_aliases(item)\n",
    "            claims = get_claims(item)\n",
    "            data_batch.append({\n",
    "                'id': item['id'],\n",
    "                'label': label,\n",
    "                'description': description,\n",
    "                'aliases': json.dumps(aliases, separators=(',', ':')),\n",
    "                'claims': json.dumps(claims, separators=(',', ':')),\n",
    "            })\n",
    "            progressbar.update(1)\n",
    "\n",
    "            process = psutil.Process()\n",
    "            progressbar.set_description(f\"Batch Size: {len(data_batch)} \\t Memory Usage: {process.memory_info().rss / 1024 ** 2:.2f} MB\")\n",
    "            with sqlitDBlock:\n",
    "                if len(data_batch) >= BATCH_SIZE:\n",
    "                    worked = WikidataEntity.add_bulk_entities(data_batch)\n",
    "                    if worked:\n",
    "                        data_batch = []\n",
    "\n",
    "async def run_processor():\n",
    "    await wikidata.run(save_entites_to_sqlite, max_iterations=None, verbose=False)\n",
    "\n",
    "await run_processor()\n",
    "\n",
    "progressbar.close()\n",
    "if len(data_batch) > 0:\n",
    "    WikidataEntity.add_bulk_entities(data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112473858/112473858 [8:49:46<00:00, 3538.45it/s]  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from wikidata_dumpreader import WikidataDumpReader\n",
    "from wikidataDB import WikidataID, WikidataEntity\n",
    "from multiprocessing import Lock\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "import psutil\n",
    "            \n",
    "FILEPATH = '../data/Wikidata/latest-all.json.bz2'\n",
    "BATCH_SIZE = 10000\n",
    "NUM_PROCESSES = 4\n",
    "skiplines = 0\n",
    "wikidata = WikidataDumpReader(FILEPATH, num_processes=NUM_PROCESSES, batch_size=BATCH_SIZE, skiplines=skiplines)\n",
    "\n",
    "def in_mul_and_not_en(item):\n",
    "    return ('sitelinks' in item) and (f'{language}wiki' in item['sitelinks']) and (((language not in item['labels']) and ('mul' in item['labels'])) or ((language not in item['descriptions']) and ('mul' in item['descriptions'])))\n",
    "\n",
    "def remove_keys(data, keys_to_remove=['hash', 'property', 'numeric-id', 'qualifiers-order']):\n",
    "    if isinstance(data, dict):\n",
    "        return {\n",
    "            key: remove_keys(value, keys_to_remove) \n",
    "            for key, value in data.items() if key not in keys_to_remove\n",
    "        }\n",
    "    elif isinstance(data, list):\n",
    "        return [remove_keys(item, keys_to_remove) for item in data]\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "def get_claims(item):\n",
    "    claims = {}\n",
    "    if 'claims' in item:\n",
    "        for pid,x in item['claims'].items():\n",
    "            pid_claims = []\n",
    "            for i in x:\n",
    "                if (i['type'] == 'statement') and (i['rank'] != 'deprecated'):\n",
    "                    pid_claims.append({\n",
    "                        'mainsnak': remove_keys(i['mainsnak']) if 'mainsnak' in i else {},\n",
    "                        'qualifiers': remove_keys(i['qualifiers']) if 'qualifiers' in i else {},\n",
    "                        'rank': i['rank']\n",
    "                    })\n",
    "            if len(pid_claims) > 0:\n",
    "                claims[pid] = pid_claims\n",
    "    return claims\n",
    "\n",
    "def get_aliases(item):\n",
    "    aliases = set()\n",
    "    if language in item['aliases']:\n",
    "        aliases = set([x['value'] for x in item['aliases'][language]])\n",
    "    if 'mul' in item['aliases']:\n",
    "        aliases = aliases | set([x['value'] for x in item['aliases']['mul']])\n",
    "    return list(aliases)\n",
    "\n",
    "data_batch = []\n",
    "progressbar = tqdm(total=112473858)\n",
    "sqlitDBlock = Lock()\n",
    "language = 'en'\n",
    "def save_entites_to_sqlite(item):\n",
    "    global data_batch\n",
    "    global missing_ids\n",
    "    global IDtypes\n",
    "\n",
    "    progressbar.update(1)\n",
    "    if item is not None:\n",
    "        if (item['id'] in missing_ids):\n",
    "            label = item['labels'][language]['value'] if (language in item['labels']) else (item['labels']['mul']['value'] if ('mul' in item['labels']) else '')\n",
    "            description = item['descriptions'][language]['value'] if (language in item['descriptions']) else (item['descriptions']['mul']['value'] if ('mul' in item['descriptions']) else '')\n",
    "            aliases = get_aliases(item)\n",
    "            claims = get_claims(item)\n",
    "            data_batch.append({\n",
    "                'id': item['id'],\n",
    "                'label': label,\n",
    "                'description': description,\n",
    "                'aliases': json.dumps(aliases, separators=(',', ':')),\n",
    "                'claims': json.dumps(claims, separators=(',', ':')),\n",
    "            })\n",
    "            \n",
    "            process = psutil.Process()\n",
    "            progressbar.set_description(f\"Batch Size: {len(data_batch)} \\t Memory Usage: {process.memory_info().rss / 1024 ** 2:.2f} MB\")\n",
    "            with sqlitDBlock:\n",
    "                if len(data_batch) >= 1:\n",
    "                    worked = WikidataEntity.add_bulk_entities(data_batch)\n",
    "                    if worked:\n",
    "                        data_batch = []\n",
    "            \n",
    "async def run_processor():\n",
    "    await wikidata.run(save_entites_to_sqlite, max_iterations=None, verbose=False)\n",
    "\n",
    "await run_processor()\n",
    "\n",
    "progressbar.close()\n",
    "if len(data_batch) > 0:\n",
    "    WikidataEntity.add_bulk_entities(data_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find IDs that are in WikidataID but not in WikidataEntity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from sqlalchemy import select\n",
    "from wikidataDB import Session, WikidataID, WikidataEntity\n",
    "\n",
    "with Session() as session:\n",
    "    result = session.execute(\n",
    "        select(WikidataID.id)\n",
    "        .outerjoin(WikidataEntity, WikidataID.id == WikidataEntity.id)\n",
    "        .filter(WikidataEntity.id == None)\n",
    "        .filter(WikidataID.in_wikipedia == True)\n",
    "    )\n",
    "    missing_ids = set(result.scalars().all())\n",
    "\n",
    "print(len(missing_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find IDs that are not in WikidataEntity but are in the claims, qualifiers, and quantity units of entities connected to Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from wikidataDB import Session, WikidataID, WikidataEntity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_missing_entities(session, ids):\n",
    "    existing_entities = session.query(WikidataEntity.id).filter(WikidataEntity.id.in_(ids)).all()\n",
    "    existing_ids = {entity.id for entity in existing_entities}\n",
    "    return set(ids) - existing_ids\n",
    "\n",
    "with Session() as session:\n",
    "    entities = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).yield_per(100000)\n",
    "\n",
    "    progressbar = tqdm(total=9203531)\n",
    "    found = False\n",
    "    missing_ids = set()\n",
    "\n",
    "    batch_size = 10000\n",
    "    ids_to_check = set()\n",
    "\n",
    "    for entity in entities:\n",
    "        progressbar.update(1)\n",
    "        for pid, claim in entity.claims.items():\n",
    "            ids_to_check.add(pid)\n",
    "            for c in claim:\n",
    "                if ('datavalue' in c['mainsnak']):\n",
    "                    if ((c['mainsnak']['datatype'] == 'wikibase-item') or (c['mainsnak']['datatype'] == 'wikibase-property')):\n",
    "                        id = c['mainsnak']['datavalue']['value']['id']\n",
    "                        ids_to_check.add(id)\n",
    "                    elif (c['mainsnak']['datatype'] == 'quantity') and (c['mainsnak']['datavalue']['value']['unit'] != '1'):\n",
    "                        id = c['mainsnak']['datavalue']['value']['unit'].rsplit('/', 1)[1]\n",
    "                        ids_to_check.add(id)\n",
    "\n",
    "                if 'qualifiers' in c:\n",
    "                    for pid, qualifier in c['qualifiers'].items():\n",
    "                        ids_to_check.add(pid)\n",
    "                        for q in qualifier:\n",
    "                            if ('datavalue' in q):\n",
    "                                if ((q['datatype'] == 'wikibase-item') or (q['datatype'] == 'wikibase-property')):\n",
    "                                    id = q['datavalue']['value']['id']\n",
    "                                    ids_to_check.add(id)\n",
    "                                elif (q['datatype'] == 'quantity') and (q['datavalue']['value']['unit'] != '1'):\n",
    "                                    id = q['datavalue']['value']['unit'].rsplit('/', 1)[1]\n",
    "                                    ids_to_check.add(id)\n",
    "\n",
    "\n",
    "        if len(ids_to_check) >= batch_size:\n",
    "            missing_ids.update(get_missing_entities(session, ids_to_check))\n",
    "            ids_to_check.clear()\n",
    "\n",
    "        if progressbar.n % 1000 == 0:\n",
    "            progressbar.set_description(f\"Missing IDs: {len(missing_ids)}\")\n",
    "\n",
    "    if ids_to_check:\n",
    "        missing_ids.update(get_missing_entities(session, ids_to_check))\n",
    "\n",
    "    progressbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

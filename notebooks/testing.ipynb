{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from wikidata_dumpreader import WikidataDumpReader\n",
    "from wikidataDB import WikidataID, WikidataEntity, Session\n",
    "from sqlalchemy import desc\n",
    "\n",
    "with Session() as session:\n",
    "    count_entity = session.query(WikidataEntity).count()\n",
    "\n",
    "with Session() as session:\n",
    "    count_id = session.query(WikidataID).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12722987"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12723412"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 81/9203531 [00:00<17:14:52, 148.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMP22, Nuclear Matrix Protein.\n",
      "Q3869592\n",
      "NWA World Tag Team Championship, Professional wrestling championship. Attributes include: \n",
      "- instance of, that class of which this subject is a particular example and member; different from P279 (subclass of); for example: K2 is an instance of mountain; volcano is a subclass of mountain (and an instance of volcanic landform): professional wrestling championship, professional wrestling competition\n",
      "Q3869701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg Token Size: 408.24, Avg Embed Size: 1.0:   0%|          | 110/9203531 [00:00<20:30:57, 124.61it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m entities:\n\u001b[1;32m     18\u001b[0m     progressbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     text \u001b[38;5;241m=\u001b[39m WikidataEmbed\u001b[38;5;241m.\u001b[39mentity_to_text(entity, with_desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m     tokens_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens_ids) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m:\n",
      "File \u001b[0;32m~/GitHub/WikidataTextEmbedding/notebooks/../src/wikidataEmbed.py:21\u001b[0m, in \u001b[0;36mWikidataEmbed.entity_to_text\u001b[0;34m(entity, with_desc)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentity_to_text\u001b[39m(entity, with_desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Converts a Wikidata entity to a readable text string, including its label, description,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    and aliases, as well as a list of its properties.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    - A string representation of the entity, its description, and its properties.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     properties \u001b[38;5;241m=\u001b[39m WikidataEmbed\u001b[38;5;241m.\u001b[39mproperties_to_text(entity\u001b[38;5;241m.\u001b[39mclaims, with_desc\u001b[38;5;241m=\u001b[39mwith_desc)\n\u001b[1;32m     22\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentity\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentity\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, also known as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(entity\u001b[38;5;241m.\u001b[39maliases)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(entity\u001b[38;5;241m.\u001b[39maliases) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/GitHub/WikidataTextEmbedding/notebooks/../src/wikidataEmbed.py:126\u001b[0m, in \u001b[0;36mWikidataEmbed.properties_to_text\u001b[0;34m(properties, with_desc)\u001b[0m\n\u001b[1;32m    123\u001b[0m         p_data\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(p_data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28mproperty\u001b[39m \u001b[38;5;241m=\u001b[39m WikidataEntity\u001b[38;5;241m.\u001b[39mget_entity(pid)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mproperty\u001b[39m:\n\u001b[1;32m    128\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mproperty\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/GitHub/WikidataTextEmbedding/notebooks/../src/wikidataDB.py:77\u001b[0m, in \u001b[0;36mWikidataEntity.get_entity\u001b[0;34m(id)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_entity\u001b[39m(\u001b[38;5;28mid\u001b[39m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 77\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mquery(WikidataEntity)\u001b[38;5;241m.\u001b[39mfilter_by(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m)\u001b[38;5;241m.\u001b[39mfirst()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sqlalchemy/orm/query.py:2728\u001b[0m, in \u001b[0;36mQuery.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2726\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter()\u001b[38;5;241m.\u001b[39mfirst()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39m_iter()\u001b[38;5;241m.\u001b[39mfirst()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/result.py:1786\u001b[0m, in \u001b[0;36mScalarResult.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[_R]:\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fetch the first object or ``None`` if no object is present.\u001b[39;00m\n\u001b[1;32m   1779\u001b[0m \n\u001b[1;32m   1780\u001b[0m \u001b[38;5;124;03m    Equivalent to :meth:`_engine.Result.first` except that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1784\u001b[0m \n\u001b[1;32m   1785\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_only_one_row(\n\u001b[1;32m   1787\u001b[0m         raise_for_second_row\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, raise_for_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, scalar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/result.py:749\u001b[0m, in \u001b[0;36mResultInternal._only_one_row\u001b[0;34m(self, raise_for_second_row, raise_for_none, scalar)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_only_one_row\u001b[39m(\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    743\u001b[0m     raise_for_second_row: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    744\u001b[0m     raise_for_none: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    745\u001b[0m     scalar: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    746\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[_R]:\n\u001b[1;32m    747\u001b[0m     onerow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetchone_impl\n\u001b[0;32m--> 749\u001b[0m     row: Optional[_InterimRowType[Any]] \u001b[38;5;241m=\u001b[39m onerow(hard_close\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m raise_for_none:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/result.py:1673\u001b[0m, in \u001b[0;36mFilterResult._fetchone_impl\u001b[0;34m(self, hard_close)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetchone_impl\u001b[39m(\n\u001b[1;32m   1671\u001b[0m     \u001b[38;5;28mself\u001b[39m, hard_close: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[_InterimRowType[Row[Any]]]:\n\u001b[0;32m-> 1673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_real_result\u001b[38;5;241m.\u001b[39m_fetchone_impl(hard_close\u001b[38;5;241m=\u001b[39mhard_close)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/result.py:2259\u001b[0m, in \u001b[0;36mIteratorResult._fetchone_impl\u001b[0;34m(self, hard_close)\u001b[0m\n\u001b[1;32m   2256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hard_closed:\n\u001b[1;32m   2257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_hard_closed()\n\u001b[0;32m-> 2259\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator, _NO_ROW)\n\u001b[1;32m   2260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;129;01mis\u001b[39;00m _NO_ROW:\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soft_close(hard\u001b[38;5;241m=\u001b[39mhard_close)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sqlalchemy/orm/loading.py:219\u001b[0m, in \u001b[0;36minstances.<locals>.chunks\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     fetch \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39m_raw_all_rows()\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_entity:\n\u001b[1;32m    222\u001b[0m     proc \u001b[38;5;241m=\u001b[39m process[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/result.py:541\u001b[0m, in \u001b[0;36mResultInternal._raw_all_rows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m make_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    540\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetchall_impl()\n\u001b[0;32m--> 541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [make_row(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows]\n",
      "File \u001b[0;32mlib/sqlalchemy/cyextension/resultproxy.pyx:22\u001b[0m, in \u001b[0;36msqlalchemy.cyextension.resultproxy.BaseRow.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mlib/sqlalchemy/cyextension/resultproxy.pyx:79\u001b[0m, in \u001b[0;36msqlalchemy.cyextension.resultproxy._apply_processors\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sqlalchemy/sql/type_api.py:2102\u001b[0m, in \u001b[0;36mTypeDecorator.result_processor.<locals>.process\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     fixed_process_value \u001b[38;5;241m=\u001b[39m process_value\n\u001b[0;32m-> 2102\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(value: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[_T]:\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fixed_process_value(value, dialect)\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m process\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from wikidataDB import Session, WikidataID, WikidataEntity\n",
    "from wikidataEmbed import WikidataEmbed\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "\n",
    "with Session() as session:\n",
    "    # Use yield_per to process large chunks of data at a time.\n",
    "    entities = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).yield_per(1000)\n",
    "    with_desc_tokens = []\n",
    "    with_desc_embeds = []\n",
    "    progressbar = tqdm(total=9203531)\n",
    "\n",
    "    for entity in entities:\n",
    "        progressbar.update(1)\n",
    "        text = WikidataEmbed.entity_to_text(entity, with_desc=True)\n",
    "        tokens_ids = tokenizer.encode(text)\n",
    "        if len(tokens_ids) < 100:\n",
    "            print(text)\n",
    "            print(entity.id)\n",
    "        with_desc_tokens.append(len(tokens_ids))\n",
    "        with_desc_embeds.append((len(tokens_ids) // 7500) +1)\n",
    "\n",
    "        # Update the progress description every 1000 iterations\n",
    "        if progressbar.n % 100 == 0:\n",
    "            progressbar.set_description(f\"Avg Token Size: {sum(with_desc_tokens)/len(with_desc_embeds)}, Avg Embed Size: {sum(with_desc_embeds)/len(with_desc_embeds)}\")\n",
    "\n",
    "    progressbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg Token Size: 408.24, Avg Embed Size: 1.0:   0%|          | 112/9203531 [05:36<7681:11:10,  3.00s/it]\n"
     ]
    }
   ],
   "source": [
    "progressbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import json\n",
    "import astrapy\n",
    "\n",
    "datastax_token = json.load(open(\"../API tokens/datastax_token.json\"))\n",
    "ASTRA_DB_APPLICATION_TOKEN = datastax_token['token']\n",
    "ASTRA_DB_API_ENDPOINT = datastax_token['endpoint']\n",
    "EMBED_DIM = 1024\n",
    "SIMILARITY_METRIC = astrapy.constants.VectorMetric.COSINE\n",
    "COLLECTION_NAME = \"wikidata_en_v1\"\n",
    "\n",
    "client = astrapy.DataAPIClient(ASTRA_DB_APPLICATION_TOKEN)\n",
    "database = client.get_database_by_api_endpoint(ASTRA_DB_API_ENDPOINT)\n",
    "\n",
    "if COLLECTION_NAME not in database.list_collection_names():\n",
    "    datastax_db = database.create_collection(\n",
    "        COLLECTION_NAME,\n",
    "        dimension=EMBED_DIM,\n",
    "        metric=SIMILARITY_METRIC\n",
    "    )\n",
    "    \n",
    "datastax_db = database.get_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/9203531 [00:11<17:45:08, 144.01it/s]"
     ]
    },
    {
     "ename": "InsertManyException",
     "evalue": "Failed to insert document with _id 'Q3868164': Document already exists with the given _id",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInsertManyException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(id_batch) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m     23\u001b[0m     vector_batch \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(text_batch, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-matching\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     datastax_db\u001b[38;5;241m.\u001b[39minsert_many(\n\u001b[1;32m     25\u001b[0m         [{\n\u001b[1;32m     26\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: qid,\n\u001b[1;32m     27\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$vector\u001b[39m\u001b[38;5;124m\"\u001b[39m: vector,\n\u001b[1;32m     28\u001b[0m         } \u001b[38;5;28;01mfor\u001b[39;00m qid,vector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(id_batch, vector_batch)\n\u001b[1;32m     29\u001b[0m         ],\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m     text_batch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     32\u001b[0m     id_batch \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/astrapy/collection.py:1021\u001b[0m, in \u001b[0;36mCollection.insert_many\u001b[0;34m(self, documents, vectors, vectorize, ordered, chunk_size, concurrency, max_time_ms)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1015\u001b[0m     [chunk_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m, []) \u001b[38;5;28;01mfor\u001b[39;00m chunk_response \u001b[38;5;129;01min\u001b[39;00m raw_results]\n\u001b[1;32m   1016\u001b[0m ):\n\u001b[1;32m   1017\u001b[0m     partial_result \u001b[38;5;241m=\u001b[39m InsertManyResult(\n\u001b[1;32m   1018\u001b[0m         raw_results\u001b[38;5;241m=\u001b[39mraw_results,\n\u001b[1;32m   1019\u001b[0m         inserted_ids\u001b[38;5;241m=\u001b[39minserted_ids,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InsertManyException\u001b[38;5;241m.\u001b[39mfrom_responses(\n\u001b[1;32m   1022\u001b[0m         commands\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m raw_results],\n\u001b[1;32m   1023\u001b[0m         raw_responses\u001b[38;5;241m=\u001b[39mraw_results,\n\u001b[1;32m   1024\u001b[0m         partial_result\u001b[38;5;241m=\u001b[39mpartial_result,\n\u001b[1;32m   1025\u001b[0m     )\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# return\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m full_result \u001b[38;5;241m=\u001b[39m InsertManyResult(\n\u001b[1;32m   1029\u001b[0m     raw_results\u001b[38;5;241m=\u001b[39mraw_results,\n\u001b[1;32m   1030\u001b[0m     inserted_ids\u001b[38;5;241m=\u001b[39minserted_ids,\n\u001b[1;32m   1031\u001b[0m )\n",
      "\u001b[0;31mInsertManyException\u001b[0m: Failed to insert document with _id 'Q3868164': Document already exists with the given _id"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from wikidataDB import Session, WikidataID, WikidataEntity\n",
    "from wikidataEmbed import WikidataEmbed\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "with Session() as session:\n",
    "    entities = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).yield_per(batch_size)\n",
    "\n",
    "    progressbar = tqdm(total=9203531)\n",
    "    text_batch = []\n",
    "    id_batch = []\n",
    "    for entity in entities:\n",
    "        progressbar.update(1)\n",
    "        text = WikidataEmbed.entity_to_text(entity, with_desc=True)\n",
    "        text_batch.append(text)\n",
    "        id_batch.append(entity.id)\n",
    "\n",
    "        if len(id_batch) >= batch_size:\n",
    "            vector_batch = model.encode(text_batch, task=\"text-matching\")\n",
    "            datastax_db.insert_many(\n",
    "                [{\n",
    "                        \"QID\": qid,\n",
    "                        \"$vector\": vector,\n",
    "                } for qid,vector in zip(id_batch, vector_batch)\n",
    "                ],\n",
    "            )\n",
    "            text_batch = []\n",
    "            id_batch = []\n",
    "            break\n",
    "    progressbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "class JinaAIEmbeddings:\n",
    "    def __init__(self, passage_task=\"retrieval.passage\", query_task=\"retrieval.query\", embedding_dim=1024):\n",
    "        self.model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True).to('cuda')\n",
    "        self.passage_task = passage_task\n",
    "        self.query_task = query_task\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, task=self.passage_task, truncate_dim=self.embedding_dim)\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.model.encode([query], task=self.query_task, truncate_dim=self.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- configuration_xlm_roberta.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- stochastic_depth.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- xlm_padding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- rotary.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- mlp.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- mha.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- block.py\n",
      "- mha.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- modeling_xlm_roberta.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- modeling_lora.py\n",
      "- embedding.py\n",
      "- stochastic_depth.py\n",
      "- xlm_padding.py\n",
      "- rotary.py\n",
      "- mlp.py\n",
      "- block.py\n",
      "- modeling_xlm_roberta.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "The 'targets_table' parameter is deprecated and will be removed in future versions.\n"
     ]
    },
    {
     "ename": "InvalidRequest",
     "evalue": "Error from server: code=2200 [Invalid query] message=\"Cosine similarity is not supported for single-dimension vectors\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequest\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m cassio\u001b[38;5;241m.\u001b[39minit(auto\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m JinaAIEmbeddings()\n\u001b[0;32m---> 19\u001b[0m graph_store \u001b[38;5;241m=\u001b[39m CassandraGraphStore(\n\u001b[1;32m     20\u001b[0m     embeddings,\n\u001b[1;32m     21\u001b[0m     node_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWikidata_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ragstack_langchain/graph_store/cassandra.py:62\u001b[0m, in \u001b[0;36mCassandraGraphStore.__init__\u001b[0;34m(self, embedding, node_table, targets_table, session, keyspace, setup_mode)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m     60\u001b[0m _setup_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(graph_store\u001b[38;5;241m.\u001b[39mSetupMode, setup_mode\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore \u001b[38;5;241m=\u001b[39m graph_store\u001b[38;5;241m.\u001b[39mGraphStore(\n\u001b[1;32m     63\u001b[0m     embedding\u001b[38;5;241m=\u001b[39m_EmbeddingModelAdapter(embedding),\n\u001b[1;32m     64\u001b[0m     node_table\u001b[38;5;241m=\u001b[39mnode_table,\n\u001b[1;32m     65\u001b[0m     targets_table\u001b[38;5;241m=\u001b[39mtargets_table,\n\u001b[1;32m     66\u001b[0m     session\u001b[38;5;241m=\u001b[39msession,\n\u001b[1;32m     67\u001b[0m     keyspace\u001b[38;5;241m=\u001b[39mkeyspace,\n\u001b[1;32m     68\u001b[0m     setup_mode\u001b[38;5;241m=\u001b[39m_setup_mode,\n\u001b[1;32m     69\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ragstack_knowledge_store/graph_store.py:187\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, embedding, node_table, targets_table, session, keyspace, setup_mode, metadata_indexing)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_by_embedding \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124m    SELECT content_id, kind, text_content, metadata_blob, links_blob\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    179\u001b[0m )\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_by_embedding\u001b[38;5;241m.\u001b[39mconsistency_level \u001b[38;5;241m=\u001b[39m ConsistencyLevel\u001b[38;5;241m.\u001b[39mONE\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_ids_and_link_to_tags_by_embedding \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124m    SELECT content_id, link_to_tags\u001b[39m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124m    FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyspace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_table\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124m    ORDER BY text_embedding ANN OF ?\u001b[39m\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;124m    LIMIT ?\u001b[39m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_ids_and_link_to_tags_by_embedding\u001b[38;5;241m.\u001b[39mconsistency_level \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    191\u001b[0m     ConsistencyLevel\u001b[38;5;241m.\u001b[39mONE\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_ids_and_link_to_tags_by_id \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124m    SELECT content_id, link_to_tags\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    200\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ragstack_knowledge_store/graph_store.py:253\u001b[0m, in \u001b[0;36m_apply_schema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Apply the schema to the database.\"\"\"\u001b[39;00m\n\u001b[1;32m    242\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding\u001b[38;5;241m.\u001b[39membed_query(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Query\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCREATE TABLE IF NOT EXISTS \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keyspace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124m        content_id TEXT,\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124m        kind TEXT,\u001b[39m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124m        text_content TEXT,\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124m        text_embedding VECTOR<FLOAT, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>,\u001b[39m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[38;5;124m        link_to_tags SET<TUPLE<TEXT, TEXT>>,\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124m        metadata_blob TEXT,\u001b[39m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124m        links_blob TEXT,\u001b[39m\n\u001b[0;32m--> 253\u001b[0m \n\u001b[1;32m    254\u001b[0m \u001b[38;5;124m        PRIMARY KEY (content_id)\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124m    )\u001b[39m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCREATE TABLE IF NOT EXISTS \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keyspace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_targets_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124m        target_content_id TEXT,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Index on text_embedding (for similarity search)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/cassandra/cluster.py:2677\u001b[0m, in \u001b[0;36mcassandra.cluster.Session.execute\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/cassandra/cluster.py:4956\u001b[0m, in \u001b[0;36mcassandra.cluster.ResponseFuture.result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mInvalidRequest\u001b[0m: Error from server: code=2200 [Invalid query] message=\"Cosine similarity is not supported for single-dimension vectors\""
     ]
    }
   ],
   "source": [
    "from typing import AsyncIterator, Iterable\n",
    "import os\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from langchain_core.documents import Document\n",
    "from ragstack_langchain.graph_store import CassandraGraphStore\n",
    "import cassio\n",
    "import json\n",
    "\n",
    "from ragstack_knowledge_store.graph_store import CONTENT_ID\n",
    "from ragstack_langchain.graph_store.extractors import HtmlInput, HtmlLinkExtractor\n",
    "from ragstack_langchain.graph_store.links import add_links\n",
    "\n",
    "datastax_token = json.load(open(\"../API tokens/datastax_token.json\"))\n",
    "os.environ[\"ASTRA_DB_DATABASE_ID\"] = datastax_token['database_id']\n",
    "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = datastax_token['token']\n",
    "\n",
    "cassio.init(auto=True)\n",
    "embeddings = JinaAIEmbeddings()\n",
    "graph_store = CassandraGraphStore(\n",
    "    embeddings,\n",
    "    node_table=f\"Wikidata_nodes\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
